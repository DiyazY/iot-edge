# TODO: remove it, obsolete!
---
- name: Kubernetes Recovery Testing
  hosts: iot_cluster  # This could target all your nodes, both worker and control plane.
  become: true
  tasks:

    - name: Create reliability test directory
      file:
        path: "~{{ ansible_user }}/reliability/{{ tag }}"
        state: directory
      when: inventory_hostname == groups['master'][0]

    - name: Get list of all nodes
      command: kubectl get nodes -o jsonpath='{.items[*].metadata.name}'
      register: node_names_output
      when: inventory_hostname == groups['master'][0]

    - name: Split node names into a list
      set_fact:
        node_names: "{{ node_names_output.stdout.split() }}"
      when: inventory_hostname == groups['master'][0]

    - name: Backup current cluster state
      shell: |
        sudo kubectl get all --all-namespaces -o yaml > ~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-backup_all.yaml
        sudo kubectl get nodes -o yaml > ~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-backup_nodes.yaml
      when: inventory_hostname == groups['master'][0]

    # - name: Wait for a specified time (e.g., 10 minutes)
    #   wait_for:
    #     timeout: 300
    #     state: paused

    - name: Get before Date and Time
      command:  date '+%s'
      register: before_datetime
      when: inventory_hostname == groups['master'][0]
  
    - name: Display the time before test
      debug:
        var: before_datetime.stdout
      when: inventory_hostname == groups['master'][0]

    - name: Wait for a specified time (e.g., 250 seconds)
      ansible.builtin.pause:
        seconds: 250

    - name: Simulate master failures (Stopping kubelet/k3s)
      systemd:
        name: k3s
        # name: kubelet
        # name: k3s-node
        state: stopped
      when: inventory_hostname == groups['master'][0]

    # - name: Simulate control plane failure (Stopping kube-apiserver)
    #   systemd:
    #     name: kube-apiserver
    #     state: stopped
    #   when: "'master' in group_names"

    - name: Wait for a specified time (e.g., 100 seconds)
      ansible.builtin.pause:
        seconds: 100

    - name: Start kubelet on worker nodes
      systemd:
        name: k3s
        # name: kubelet
        # name: k3s-node
        state: started
      when: inventory_hostname == groups['master'][0]

    # - name: Start kube-apiserver on control plane
    #   systemd:
    #     name: kube-apiserver
    #     state: started
    #   when: "'k8s_control_plane' in group_names"

    # - name: Wait for cluster components to stabilize
    #   wait_for:
    #     timeout: 300
        # state: paused


    - name: Wait until all nodes are Ready
      command: kubectl wait --for=condition=Ready node/{{ item }} --timeout=5m # issue with k3s that it can arbitrarily show nodes as NotReady
      with_items: "{{ node_names }}" 
      when: inventory_hostname == groups['master'][0]


    - name: Wait for a specified time (e.g., 600 seconds)
      ansible.builtin.pause:
        seconds: 600


    - name: Get after Date and Time
      command:  date '+%s'
      register: after_datetime
      when: inventory_hostname == groups['master'][0]
      
    - name: Display the time after test
      debug:
        var: after_datetime.stdout
      when: inventory_hostname == groups['master'][0]

    - name: Calculate recovery duration
      set_fact:
        recovery_duration: "{{ (after_datetime.stdout | int) - (before_datetime.stdout | int) }}"
      when: inventory_hostname == groups['master'][0]
      
    - name: Display recovery duration
      debug:
        var: recovery_duration
      when: inventory_hostname == groups['master'][0]
      

    # - name: Measure recovery time
    #   delegate_to: localhost
    #   block:
    #     - name: Record start time
    #       set_fact:
    #         recovery_start_time: "{{ lookup('pipe','date +%s') }}"
        
    #     - name: Wait until all nodes are Ready
    #       command: kubectl wait --for=condition=Ready node/{{ item }} --timeout=5m
    #       with_items: "{{ groups['iot_cluster'] }}"  # Adjust if you have a different group name for all nodes.

    #     - name: Record end time
    #       set_fact:
    #         recovery_end_time: "{{ lookup('pipe','date +%s') }}"
        
    #     - name: Calculate recovery duration
    #       set_fact:
    #         recovery_duration: "{{ (recovery_end_time | int) - (recovery_start_time | int) }}"
        
    #     - name: Display recovery duration
    #       debug:
    #         var: recovery_duration

    - name: Gather post-recovery cluster state
      shell: |
        sudo kubectl get all --all-namespaces -o yaml > ~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-post_recovery_all.yaml
        sudo kubectl get nodes -o yaml > ~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-post_recovery_nodes.yaml
      when: inventory_hostname == groups['master'][0]

    - name: Copy test results to repository
      fetch:
        src: "~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-{{item.file}}.yaml"
        dest: "../../reliability/{{k8s_distribution}}/{{ tag }}/"
        flat: yes
      loop:
        - file: backup_nodes
        - file : backup_all
        - file : post_recovery_nodes
        - file : post_recovery_all
      when: inventory_hostname == groups['master'][0]
      ignore_errors: true
    
    - name: Delete test results on a remote machine
      file:
        path: "~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-{{item.file}}.yaml"
        state: absent
      loop:
        - file: backup_nodes
        - file : backup_all
        - file : post_recovery_nodes
        - file : post_recovery_all
      when: inventory_hostname == groups['master'][0]
      ignore_errors: true

