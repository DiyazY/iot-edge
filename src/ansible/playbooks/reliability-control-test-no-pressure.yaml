---
- name: Kubernetes Recovery Testing
  hosts: iot_cluster  # This could target all your nodes, both worker and control plane.
  become: true
  tasks:

    - name: Create reliability test directory
      file:
        path: "~{{ ansible_user }}/reliability/{{ tag }}"
        state: directory
      when: inventory_hostname == groups['master'][0]

    - name: Get list of all nodes
      command: kubectl get nodes -o jsonpath='{.items[*].metadata.name}'
      register: node_names_output
      when: inventory_hostname == groups['master'][0]

    - name: Split node names into a list
      set_fact:
        node_names: "{{ node_names_output.stdout.split() }}"
      when: inventory_hostname == groups['master'][0]

    - name: Backup current cluster state
      shell: |
        sudo kubectl get all --all-namespaces -o yaml > ~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-backup_all.yaml
        sudo kubectl get nodes -o yaml > ~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-backup_nodes.yaml
      when: inventory_hostname == groups['master'][0]

    - name: Get before Date and Time
      command:  date '+%s'
      register: before_datetime
      when: inventory_hostname == groups['master'][0]
  
    - name: Display the time before test
      debug:
        var: before_datetime.stdout
      when: inventory_hostname == groups['master'][0]

    - name: Wait for a specified time (e.g., 250 seconds)
      ansible.builtin.pause:
        seconds: 250

    - name: Simulate worker failure (Stopping network interface)
      shell: |
        ip link set dev eno1 down
        (sleep 500 && ip link set dev eno1 up) &
      when: inventory_hostname == groups['master'][0]

    - name: Wait until all nodes are Ready
      command: kubectl wait --for=condition=Ready node/{{ item }} --timeout=5m # issue with k3s that it can arbitrarily show nodes as NotReady
      with_items: "{{ node_names }}" 
      when: inventory_hostname == groups['master'][0]

    - name: Wait for a specified time (e.g., 250 seconds)
      ansible.builtin.pause:
        seconds: 250

    - name: Get after Date and Time
      command:  date '+%s'
      register: after_datetime
      when: inventory_hostname == groups['master'][0]
      
    - name: Display the time after test
      debug:
        var: after_datetime.stdout
      when: inventory_hostname == groups['master'][0]

    - name: Calculate recovery duration
      set_fact:
        recovery_duration: "{{ (after_datetime.stdout | int) - (before_datetime.stdout | int) }}"
      when: inventory_hostname == groups['master'][0]
      
    - name: Display recovery duration
      debug:
        var: recovery_duration
      when: inventory_hostname == groups['master'][0]

    - name: Gather post-recovery cluster state
      shell: |
        sudo kubectl get all --all-namespaces -o yaml > ~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-post_recovery_all.yaml
        sudo kubectl get nodes -o yaml > ~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-post_recovery_nodes.yaml
      when: inventory_hostname == groups['master'][0]

    - name: Copy test results to repository
      fetch:
        src: "~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-{{item.file}}.yaml"
        dest: "../../reliability/{{k8s_distribution}}/{{ tag }}/"
        flat: yes
      loop:
        - file: backup_nodes
        - file : backup_all
        - file : post_recovery_nodes
        - file : post_recovery_all
      when: inventory_hostname == groups['master'][0]
      ignore_errors: true
    
    - name: Delete test results on a remote machine
      file:
        path: "~{{ ansible_user }}/reliability/{{ tag }}/{{tag}}-{{item.file}}.yaml"
        state: absent
      loop:
        - file: backup_nodes
        - file : backup_all
        - file : post_recovery_nodes
        - file : post_recovery_all
      when: inventory_hostname == groups['master'][0]
      ignore_errors: true

