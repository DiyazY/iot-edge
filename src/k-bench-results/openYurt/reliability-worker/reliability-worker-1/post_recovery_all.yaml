apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-21T01:36:04Z"
    labels:
      app: kbench
      name: myredisserverclient
      opnum: "0"
      podtype: redisworker
      tid: "0"
      type: kbench-pod
    name: kbench-pod-oid-0-tid-0
    namespace: kbench-pod-namespace
    resourceVersion: "1653061"
    uid: 2ad5bd27-9a81-4bbc-a0d4-33ead601aae0
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: my.kubernetes.io/instance-type
              operator: In
              values:
              - worker
    containers:
    - args:
      - apt-get update; apt-get install -y redis-server; apt-get install -y git libssl-dev
        build-essential autoconf automake libpcre3-dev libevent-dev pkg-config zlib1g-dev;
        cd /; git clone https://github.com/RedisLabs/memtier_benchmark.git; cd /memtier_benchmark/;
        autoreconf -ivf; ./configure; make; sleep infinity;
      command:
      - /bin/sh
      - -c
      image: nginx
      imagePullPolicy: Always
      name: rediscontainer
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wcn8t
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-75e4c5
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-wcn8t
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://797606950e1df12739b132fcca360e3035da450920c7956a1582742169b35d14
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:6db391d1c0cfb30588ba0bf72ea999404f2764febf0f1f196acd5867ac7efa7e
      lastState: {}
      name: rediscontainer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-21T01:36:06Z"
    hostIP: 192.168.1.100
    hostIPs:
    - ip: 192.168.1.100
    phase: Running
    podIP: 192.168.1.100
    podIPs:
    - ip: 192.168.1.100
    qosClass: BestEffort
    startTime: "2024-03-21T01:36:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-21T01:36:04Z"
    labels:
      app: kbench
      name: myredisserverclient
      opnum: "0"
      podtype: redisworker
      tid: "1"
      type: kbench-pod
    name: kbench-pod-oid-0-tid-1
    namespace: kbench-pod-namespace
    resourceVersion: "1652255"
    uid: 64f67479-9ea1-4e9f-bdd1-aead1fb94a5c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: my.kubernetes.io/instance-type
              operator: In
              values:
              - worker
    containers:
    - args:
      - apt-get update; apt-get install -y redis-server; apt-get install -y git libssl-dev
        build-essential autoconf automake libpcre3-dev libevent-dev pkg-config zlib1g-dev;
        cd /; git clone https://github.com/RedisLabs/memtier_benchmark.git; cd /memtier_benchmark/;
        autoreconf -ivf; ./configure; make; sleep infinity;
      command:
      - /bin/sh
      - -c
      image: nginx
      imagePullPolicy: Always
      name: rediscontainer
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-64t7q
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-76d48b
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-64t7q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:06Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:06Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a87ffff454e1b3d207bf41a35da208be5003d7352914c109b91f7cc35443c805
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:6db391d1c0cfb30588ba0bf72ea999404f2764febf0f1f196acd5867ac7efa7e
      lastState: {}
      name: rediscontainer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-21T01:36:06Z"
    hostIP: 192.168.1.101
    hostIPs:
    - ip: 192.168.1.101
    phase: Running
    podIP: 192.168.1.101
    podIPs:
    - ip: 192.168.1.101
    qosClass: BestEffort
    startTime: "2024-03-21T01:36:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-21T01:36:04Z"
    labels:
      app: kbench
      name: myredisserverclient
      opnum: "0"
      podtype: redisworker
      tid: "2"
      type: kbench-pod
    name: kbench-pod-oid-0-tid-2
    namespace: kbench-pod-namespace
    resourceVersion: "1652266"
    uid: ad4dea0b-e216-4c73-b4ce-66468d0a8b1b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: my.kubernetes.io/instance-type
              operator: In
              values:
              - worker
    containers:
    - args:
      - apt-get update; apt-get install -y redis-server; apt-get install -y git libssl-dev
        build-essential autoconf automake libpcre3-dev libevent-dev pkg-config zlib1g-dev;
        cd /; git clone https://github.com/RedisLabs/memtier_benchmark.git; cd /memtier_benchmark/;
        autoreconf -ivf; ./configure; make; sleep infinity;
      command:
      - /bin/sh
      - -c
      image: nginx
      imagePullPolicy: Always
      name: rediscontainer
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mxcvh
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-a6773f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-mxcvh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-21T01:36:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://23a3a3c06dd6b76d23cab17ae0f6742060baa583219b73f0ca9701aa3f9a24a2
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:6db391d1c0cfb30588ba0bf72ea999404f2764febf0f1f196acd5867ac7efa7e
      lastState: {}
      name: rediscontainer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-21T01:36:08Z"
    hostIP: 192.168.1.102
    hostIPs:
    - ip: 192.168.1.102
    phase: Running
    podIP: 192.168.1.102
    podIPs:
    - ip: 192.168.1.102
    qosClass: BestEffort
    startTime: "2024-03-21T01:36:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T15:06:18Z"
    generateName: kube-flannel-ds-
    labels:
      app: flannel
      controller-revision-hash: 655c87b4c5
      k8s-app: flannel
      pod-template-generation: "3"
      tier: node
    name: kube-flannel-ds-m6zhs
    namespace: kube-flannel
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-flannel-ds
      uid: e54a963e-05f8-41d2-83b2-f61755a3cbd7
    resourceVersion: "1653059"
    uid: e9a6957f-f14b-4da7-975e-991603f4c49c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-75e4c5
    containers:
    - args:
      - --ip-masq
      - --kube-subnet-mgr
      command:
      - /opt/bin/flanneld
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: EVENT_QUEUE_DEPTH
        value: "5000"
      image: docker.io/flannel/flannel:v0.24.3
      imagePullPolicy: IfNotPresent
      name: kube-flannel
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/flannel
        name: run
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s5l58
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - -f
      - /flannel
      - /opt/cni/bin/flannel
      command:
      - cp
      image: docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
      imagePullPolicy: IfNotPresent
      name: install-cni-plugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s5l58
        readOnly: true
    - args:
      - -f
      - /etc/kube-flannel/cni-conf.json
      - /etc/cni/net.d/10-flannel.conflist
      command:
      - cp
      image: docker.io/flannel/flannel:v0.24.3
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s5l58
        readOnly: true
    nodeName: ubuntu-host-75e4c5
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: flannel
    serviceAccountName: flannel
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/flannel
        type: ""
      name: run
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - configMap:
        defaultMode: 420
        name: kube-flannel-cfg
      name: flannel-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: kube-api-access-s5l58
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T15:06:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T15:06:18Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://631729320937005aa8a04f5bf38a9b334fe2ff1c8510439cd4fcb25e862b826b
      image: docker.io/flannel/flannel:v0.24.3
      imageID: docker.io/flannel/flannel@sha256:452061a392663283672e905be10762e142d7ad6126ddee7b772e14405ee79a6a
      lastState:
        terminated:
          containerID: containerd://a242b3dd1c8d0ad9443145b74900743d2c63dad319abccc820bb7bb7fee04af1
          exitCode: 255
          finishedAt: "2023-09-19T17:00:20Z"
          reason: Unknown
          startedAt: "2024-03-17T15:06:21Z"
      name: kube-flannel
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:55:11Z"
    hostIP: 192.168.1.100
    hostIPs:
    - ip: 192.168.1.100
    initContainerStatuses:
    - containerID: containerd://dd41cc7f35d0ca23dec23f19fe32adabff8fac3912d13fb2c218982ef3a9066e
      image: docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
      imageID: docker.io/flannel/flannel-cni-plugin@sha256:743c25e5e477527d8e54faa3e5259fbbee3463a335de1690879fc74305edc79b
      lastState: {}
      name: install-cni-plugin
      ready: true
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://dd41cc7f35d0ca23dec23f19fe32adabff8fac3912d13fb2c218982ef3a9066e
          exitCode: 0
          finishedAt: "2024-03-19T15:55:05Z"
          reason: Completed
          startedAt: "2024-03-19T15:55:05Z"
    - containerID: containerd://e01c7ff12a372c5324b26700a300fbdf2c2d5655f6ec71888abef82555357525
      image: docker.io/flannel/flannel:v0.24.3
      imageID: docker.io/flannel/flannel@sha256:452061a392663283672e905be10762e142d7ad6126ddee7b772e14405ee79a6a
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e01c7ff12a372c5324b26700a300fbdf2c2d5655f6ec71888abef82555357525
          exitCode: 0
          finishedAt: "2024-03-19T15:55:09Z"
          reason: Completed
          startedAt: "2024-03-19T15:55:09Z"
    phase: Running
    podIP: 192.168.1.100
    podIPs:
    - ip: 192.168.1.100
    qosClass: Burstable
    startTime: "2024-03-17T15:06:18Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T12:41:45Z"
    generateName: kube-flannel-ds-
    labels:
      app: flannel
      controller-revision-hash: 655c87b4c5
      k8s-app: flannel
      pod-template-generation: "3"
      tier: node
    name: kube-flannel-ds-q9pmf
    namespace: kube-flannel
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-flannel-ds
      uid: e54a963e-05f8-41d2-83b2-f61755a3cbd7
    resourceVersion: "1397252"
    uid: 8e72637c-07e3-4312-90bc-aa06b0f7cbf2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-ddde9b
    containers:
    - args:
      - --ip-masq
      - --kube-subnet-mgr
      command:
      - /opt/bin/flanneld
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: EVENT_QUEUE_DEPTH
        value: "5000"
      image: docker.io/flannel/flannel:v0.24.3
      imagePullPolicy: IfNotPresent
      name: kube-flannel
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/flannel
        name: run
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h9q5c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - -f
      - /flannel
      - /opt/cni/bin/flannel
      command:
      - cp
      image: docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
      imagePullPolicy: IfNotPresent
      name: install-cni-plugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h9q5c
        readOnly: true
    - args:
      - -f
      - /etc/kube-flannel/cni-conf.json
      - /etc/cni/net.d/10-flannel.conflist
      command:
      - cp
      image: docker.io/flannel/flannel:v0.24.3
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h9q5c
        readOnly: true
    nodeName: ubuntu-host-ddde9b
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: flannel
    serviceAccountName: flannel
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/flannel
        type: ""
      name: run
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - configMap:
        defaultMode: 420
        name: kube-flannel-cfg
      name: flannel-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: kube-api-access-h9q5c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T12:41:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T12:41:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5a0d607c3fad9f9fbd11f7d94a61d90e189c014b4f11a0eba45d121d9998158f
      image: docker.io/flannel/flannel:v0.24.3
      imageID: docker.io/flannel/flannel@sha256:452061a392663283672e905be10762e142d7ad6126ddee7b772e14405ee79a6a
      lastState:
        terminated:
          containerID: containerd://9a76f19c9c6010e99d7478c8ee40311e0ee0400db412271538aa00803c8d987d
          exitCode: 255
          finishedAt: "2024-03-19T15:52:06Z"
          reason: Unknown
          startedAt: "2024-03-17T14:46:28Z"
      name: kube-flannel
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:52:25Z"
    hostIP: 192.168.1.106
    hostIPs:
    - ip: 192.168.1.106
    initContainerStatuses:
    - containerID: containerd://35102ab674e61a705ad5287fed15b0e430ed0294214570b68e15711009838d42
      image: docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
      imageID: docker.io/flannel/flannel-cni-plugin@sha256:743c25e5e477527d8e54faa3e5259fbbee3463a335de1690879fc74305edc79b
      lastState: {}
      name: install-cni-plugin
      ready: true
      restartCount: 2
      started: false
      state:
        terminated:
          containerID: containerd://35102ab674e61a705ad5287fed15b0e430ed0294214570b68e15711009838d42
          exitCode: 0
          finishedAt: "2024-03-19T15:52:24Z"
          reason: Completed
          startedAt: "2024-03-19T15:52:24Z"
    - containerID: containerd://93aaf30d63f5d03293b82e68c3128d4644e974e47693a497876c09eb5b77cfcd
      image: docker.io/flannel/flannel:v0.24.3
      imageID: docker.io/flannel/flannel@sha256:452061a392663283672e905be10762e142d7ad6126ddee7b772e14405ee79a6a
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://93aaf30d63f5d03293b82e68c3128d4644e974e47693a497876c09eb5b77cfcd
          exitCode: 0
          finishedAt: "2024-03-19T15:52:24Z"
          reason: Completed
          startedAt: "2024-03-19T15:52:24Z"
    phase: Running
    podIP: 192.168.1.106
    podIPs:
    - ip: 192.168.1.106
    qosClass: Burstable
    startTime: "2024-03-17T12:41:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T15:04:40Z"
    generateName: kube-flannel-ds-
    labels:
      app: flannel
      controller-revision-hash: 655c87b4c5
      k8s-app: flannel
      pod-template-generation: "3"
      tier: node
    name: kube-flannel-ds-rp45k
    namespace: kube-flannel
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-flannel-ds
      uid: e54a963e-05f8-41d2-83b2-f61755a3cbd7
    resourceVersion: "1397918"
    uid: 2bcbdf78-7d2e-45b6-9356-d795f6e53347
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-a6773f
    containers:
    - args:
      - --ip-masq
      - --kube-subnet-mgr
      command:
      - /opt/bin/flanneld
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: EVENT_QUEUE_DEPTH
        value: "5000"
      image: docker.io/flannel/flannel:v0.24.3
      imagePullPolicy: IfNotPresent
      name: kube-flannel
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/flannel
        name: run
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rjf7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - -f
      - /flannel
      - /opt/cni/bin/flannel
      command:
      - cp
      image: docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
      imagePullPolicy: IfNotPresent
      name: install-cni-plugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rjf7
        readOnly: true
    - args:
      - -f
      - /etc/kube-flannel/cni-conf.json
      - /etc/cni/net.d/10-flannel.conflist
      command:
      - cp
      image: docker.io/flannel/flannel:v0.24.3
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rjf7
        readOnly: true
    nodeName: ubuntu-host-a6773f
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: flannel
    serviceAccountName: flannel
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/flannel
        type: ""
      name: run
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - configMap:
        defaultMode: 420
        name: kube-flannel-cfg
      name: flannel-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: kube-api-access-6rjf7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:14Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T15:05:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T15:04:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://63069e0e0adcded3490dacd34fba7f4acda5317dd4f190bbc3c8b6b411c54c73
      image: docker.io/flannel/flannel:v0.24.3
      imageID: docker.io/flannel/flannel@sha256:452061a392663283672e905be10762e142d7ad6126ddee7b772e14405ee79a6a
      lastState:
        terminated:
          containerID: containerd://0b79d0da59fe7e56cf1dfbdfbd3caca77922248d3d10536da612d7ccd3192554
          exitCode: 255
          finishedAt: "2023-09-19T17:00:22Z"
          reason: Unknown
          startedAt: "2024-03-17T15:05:11Z"
      name: kube-flannel
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:55:24Z"
    hostIP: 192.168.1.102
    hostIPs:
    - ip: 192.168.1.102
    initContainerStatuses:
    - containerID: containerd://5fbdf2384c569840aacdac8e73e4f12da7b767e853e7395cad902a4faf338f34
      image: docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
      imageID: docker.io/flannel/flannel-cni-plugin@sha256:743c25e5e477527d8e54faa3e5259fbbee3463a335de1690879fc74305edc79b
      lastState: {}
      name: install-cni-plugin
      ready: true
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://5fbdf2384c569840aacdac8e73e4f12da7b767e853e7395cad902a4faf338f34
          exitCode: 0
          finishedAt: "2024-03-19T15:55:15Z"
          reason: Completed
          startedAt: "2024-03-19T15:55:13Z"
    - containerID: containerd://a30feef087dbad9e6d1c57c6709d872165b2e90ebc565ddda6c4ebdadf9f9793
      image: docker.io/flannel/flannel:v0.24.3
      imageID: docker.io/flannel/flannel@sha256:452061a392663283672e905be10762e142d7ad6126ddee7b772e14405ee79a6a
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a30feef087dbad9e6d1c57c6709d872165b2e90ebc565ddda6c4ebdadf9f9793
          exitCode: 0
          finishedAt: "2024-03-19T15:55:21Z"
          reason: Completed
          startedAt: "2024-03-19T15:55:20Z"
    phase: Running
    podIP: 192.168.1.102
    podIPs:
    - ip: 192.168.1.102
    qosClass: Burstable
    startTime: "2024-03-17T15:04:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T14:56:49Z"
    generateName: kube-flannel-ds-
    labels:
      app: flannel
      controller-revision-hash: 655c87b4c5
      k8s-app: flannel
      pod-template-generation: "3"
      tier: node
    name: kube-flannel-ds-wwds5
    namespace: kube-flannel
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-flannel-ds
      uid: e54a963e-05f8-41d2-83b2-f61755a3cbd7
    resourceVersion: "1397948"
    uid: 158cfef9-5e02-4cd1-8a9d-5d726ae7d078
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-76d48b
    containers:
    - args:
      - --ip-masq
      - --kube-subnet-mgr
      command:
      - /opt/bin/flanneld
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: EVENT_QUEUE_DEPTH
        value: "5000"
      image: docker.io/flannel/flannel:v0.24.3
      imagePullPolicy: IfNotPresent
      name: kube-flannel
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/flannel
        name: run
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hdfv5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - -f
      - /flannel
      - /opt/cni/bin/flannel
      command:
      - cp
      image: docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
      imagePullPolicy: IfNotPresent
      name: install-cni-plugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hdfv5
        readOnly: true
    - args:
      - -f
      - /etc/kube-flannel/cni-conf.json
      - /etc/cni/net.d/10-flannel.conflist
      command:
      - cp
      image: docker.io/flannel/flannel:v0.24.3
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hdfv5
        readOnly: true
    nodeName: ubuntu-host-76d48b
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: flannel
    serviceAccountName: flannel
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/flannel
        type: ""
      name: run
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - configMap:
        defaultMode: 420
        name: kube-flannel-cfg
      name: flannel-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: kube-api-access-hdfv5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T14:57:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T14:56:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5397033c78d0d7b43c30ab329b1fcafb1b7bace3aefd427240bed4858cde2c60
      image: docker.io/flannel/flannel:v0.24.3
      imageID: docker.io/flannel/flannel@sha256:452061a392663283672e905be10762e142d7ad6126ddee7b772e14405ee79a6a
      lastState:
        terminated:
          containerID: containerd://22e0e5601efb3ef1ab4be754e28ad1e763c893116703d7cc7cac85767d6f85ea
          exitCode: 255
          finishedAt: "2023-09-19T17:00:23Z"
          reason: Unknown
          startedAt: "2024-03-17T14:57:01Z"
      name: kube-flannel
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:55:32Z"
    hostIP: 192.168.1.101
    hostIPs:
    - ip: 192.168.1.101
    initContainerStatuses:
    - containerID: containerd://6ee9e840b3f7eaeef49b48da071498e522470176eac928c1cef09ee053356e5a
      image: docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
      imageID: docker.io/flannel/flannel-cni-plugin@sha256:743c25e5e477527d8e54faa3e5259fbbee3463a335de1690879fc74305edc79b
      lastState: {}
      name: install-cni-plugin
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://6ee9e840b3f7eaeef49b48da071498e522470176eac928c1cef09ee053356e5a
          exitCode: 0
          finishedAt: "2024-03-19T15:55:23Z"
          reason: Completed
          startedAt: "2024-03-19T15:55:22Z"
    - containerID: containerd://820487864321bef36778461ed3cca43bd1105a607c5ab91e6af425ddd23af969
      image: docker.io/flannel/flannel:v0.24.3
      imageID: docker.io/flannel/flannel@sha256:452061a392663283672e905be10762e142d7ad6126ddee7b772e14405ee79a6a
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://820487864321bef36778461ed3cca43bd1105a607c5ab91e6af425ddd23af969
          exitCode: 0
          finishedAt: "2024-03-19T15:55:29Z"
          reason: Completed
          startedAt: "2024-03-19T15:55:28Z"
    phase: Running
    podIP: 192.168.1.101
    podIPs:
    - ip: 192.168.1.101
    qosClass: Burstable
    startTime: "2024-03-17T14:56:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-09T20:51:27Z"
    generateName: coredns-76f75df574-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 76f75df574
    name: coredns-76f75df574-4v8wl
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-76f75df574
      uid: 6ca83217-cda2-4947-a447-8924ff57dff6
    resourceVersion: "1634306"
    uid: e4a49dd2-cb1c-4ac1-b37a-94c403904dfe
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-26jmh
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: ubuntu-host-ddde9b
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-26jmh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:28Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-09T20:52:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:13:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:13:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-09T20:52:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a6e9bbeb42d0b0862dc668f4db26dd1d370916899e1154ee2535b187ea885ba5
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imageID: registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1
      lastState:
        terminated:
          containerID: containerd://0053b8bc1b7ad0ffeb82ab4d4e1e5050de5a0140a4c6773b627d1072b3efb01d
          exitCode: 0
          finishedAt: "2024-03-20T23:12:02Z"
          reason: Completed
          startedAt: "2024-03-20T21:13:22Z"
      name: coredns
      ready: true
      restartCount: 46
      started: true
      state:
        running:
          startedAt: "2024-03-20T23:12:02Z"
    hostIP: 192.168.1.106
    hostIPs:
    - ip: 192.168.1.106
    phase: Running
    podIP: 10.244.0.24
    podIPs:
    - ip: 10.244.0.24
    qosClass: Burstable
    startTime: "2024-03-09T20:52:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-09T20:51:27Z"
    generateName: coredns-76f75df574-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 76f75df574
    name: coredns-76f75df574-mh7j6
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-76f75df574
      uid: 6ca83217-cda2-4947-a447-8924ff57dff6
    resourceVersion: "1634278"
    uid: 3fb83f86-dfaa-4703-8a4e-f2f29e7ebc28
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kbs28
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: ubuntu-host-ddde9b
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-kbs28
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:28Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-09T20:52:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:13:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:13:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-09T20:52:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://fe317e1df7e647e23b20502a4213851aa2aa0f16069dd3a7b83cd9a150c75d5f
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imageID: registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1
      lastState:
        terminated:
          containerID: containerd://110d6ca26f47c1fc68954c61234ec3f4ade9abd2825ec819ded1e3e1ccf3f6b7
          exitCode: 0
          finishedAt: "2024-03-20T23:11:56Z"
          reason: Completed
          startedAt: "2024-03-20T21:13:26Z"
      name: coredns
      ready: true
      restartCount: 46
      started: true
      state:
        running:
          startedAt: "2024-03-20T23:11:56Z"
    hostIP: 192.168.1.106
    hostIPs:
    - ip: 192.168.1.106
    phase: Running
    podIP: 10.244.0.25
    podIPs:
    - ip: 10.244.0.25
    qosClass: Burstable
    startTime: "2024-03-09T20:52:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.1.106:2379
      kubernetes.io/config.hash: 18bdb5bd0135b72d2ccca31748783ae6
      kubernetes.io/config.mirror: 18bdb5bd0135b72d2ccca31748783ae6
      kubernetes.io/config.seen: "2024-03-09T22:50:35.738176482+02:00"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-03-09T20:51:09Z"
    labels:
      component: etcd
      tier: control-plane
    name: etcd-ubuntu-host-ddde9b
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: ubuntu-host-ddde9b
      uid: ab930fb7-baf3-4da2-be31-c1634bbe0eb8
    resourceVersion: "1397340"
    uid: 80db9d8d-54bf-4642-8a0c-ef7fac9e0588
  spec:
    containers:
    - command:
      - etcd
      - --advertise-client-urls=https://192.168.1.106:2379
      - --cert-file=/etc/kubernetes/pki/etcd/server.crt
      - --client-cert-auth=true
      - --data-dir=/var/lib/etcd
      - --experimental-initial-corrupt-check=true
      - --experimental-watch-progress-notify-interval=5s
      - --initial-advertise-peer-urls=https://192.168.1.106:2380
      - --initial-cluster=ubuntu-host-ddde9b=https://192.168.1.106:2380
      - --key-file=/etc/kubernetes/pki/etcd/server.key
      - --listen-client-urls=https://127.0.0.1:2379,https://192.168.1.106:2379
      - --listen-metrics-urls=http://127.0.0.1:2381
      - --listen-peer-urls=https://192.168.1.106:2380
      - --name=ubuntu-host-ddde9b
      - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      - --peer-client-cert-auth=true
      - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      - --snapshot-count=10000
      - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      image: registry.k8s.io/etcd:3.5.10-0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /health?exclude=NOSPACE&serializable=true
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: etcd
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /health?serializable=false
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/etcd
        name: etcd-data
      - mountPath: /etc/kubernetes/pki/etcd
        name: etcd-certs
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-ddde9b
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/pki/etcd
        type: DirectoryOrCreate
      name: etcd-certs
    - hostPath:
        path: /var/lib/etcd
        type: DirectoryOrCreate
      name: etcd-data
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9515734a8202b2958e0fd29d06f62919ab810ea138b604f5ecff1371dab06026
      image: registry.k8s.io/etcd:3.5.10-0
      imageID: registry.k8s.io/etcd@sha256:22f892d7672adc0b9c86df67792afdb8b2dc08880f49f669eaaa59c47d7908c2
      lastState:
        terminated:
          containerID: containerd://03feb503db81fe8d4898031651dae86f5434474472eafe86dac6943ef6b0b3e2
          exitCode: 255
          finishedAt: "2024-03-19T15:52:06Z"
          reason: Unknown
          startedAt: "2024-03-17T14:46:24Z"
      name: etcd
      ready: true
      restartCount: 26
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:52:21Z"
    hostIP: 192.168.1.106
    hostIPs:
    - ip: 192.168.1.106
    phase: Running
    podIP: 192.168.1.106
    podIPs:
    - ip: 192.168.1.106
    qosClass: Burstable
    startTime: "2024-03-19T15:52:20Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.1.106:6443
      kubernetes.io/config.hash: 0448e241a9cd75a937ceb9ab1d0eece7
      kubernetes.io/config.mirror: 0448e241a9cd75a937ceb9ab1d0eece7
      kubernetes.io/config.seen: "2024-03-09T22:50:35.738177651+02:00"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-03-09T20:51:09Z"
    labels:
      component: kube-apiserver
      tier: control-plane
    name: kube-apiserver-ubuntu-host-ddde9b
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: ubuntu-host-ddde9b
      uid: ab930fb7-baf3-4da2-be31-c1634bbe0eb8
    resourceVersion: "1634233"
    uid: 8e063f79-1e9d-4e25-8ac5-8e9c04c73f37
  spec:
    containers:
    - command:
      - kube-apiserver
      - --advertise-address=192.168.1.106
      - --allow-privileged=true
      - --authorization-mode=Node,RBAC
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --enable-admission-plugins=NodeRestriction
      - --enable-bootstrap-token-auth=true
      - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      - --etcd-servers=https://127.0.0.1:2379
      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      - --requestheader-allowed-names=front-proxy-client
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --requestheader-extra-headers-prefix=X-Remote-Extra-
      - --requestheader-group-headers=X-Remote-Group
      - --requestheader-username-headers=X-Remote-User
      - --secure-port=6443
      - --service-account-issuer=https://kubernetes.default.svc.cluster.local
      - --service-account-key-file=/etc/kubernetes/pki/sa.pub
      - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/12
      - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
      image: registry.k8s.io/kube-apiserver:v1.29.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 192.168.1.106
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-apiserver
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 192.168.1.106
          path: /readyz
          port: 6443
          scheme: HTTPS
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 15
      resources:
        requests:
          cpu: 250m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 192.168.1.106
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /etc/pki
        name: etc-pki
        readOnly: true
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-ddde9b
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /etc/pki
        type: DirectoryOrCreate
      name: etc-pki
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:13:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:13:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://43aadde7d65c69b79bc17629c80c932f61680ec1f7fe6748e8b3a3c5f88ad62b
      image: registry.k8s.io/kube-apiserver:v1.29.2
      imageID: registry.k8s.io/kube-apiserver@sha256:fe4196cd9fa06bd75b5fb437be89bbccc277e83f3e0296c30b71485ce4834461
      lastState:
        terminated:
          containerID: containerd://974d8d9e1f1b886f9d29de34098d2adb24bebdad6f24a3a4e9faeb61ee2377a9
          exitCode: 137
          finishedAt: "2024-03-20T23:12:55Z"
          reason: Error
          startedAt: "2024-03-20T21:14:15Z"
      name: kube-apiserver
      ready: true
      restartCount: 78
      started: true
      state:
        running:
          startedAt: "2024-03-20T23:12:55Z"
    hostIP: 192.168.1.106
    hostIPs:
    - ip: 192.168.1.106
    phase: Running
    podIP: 192.168.1.106
    podIPs:
    - ip: 192.168.1.106
    qosClass: Burstable
    startTime: "2024-03-19T15:52:20Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 33c71c4a090b5c973a9fb47430f64892
      kubernetes.io/config.mirror: 33c71c4a090b5c973a9fb47430f64892
      kubernetes.io/config.seen: "2024-03-17T12:31:47.655858892+02:00"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-03-17T10:31:59Z"
    labels:
      component: kube-controller-manager
      tier: control-plane
    name: kube-controller-manager-ubuntu-host-ddde9b
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: ubuntu-host-ddde9b
      uid: ab930fb7-baf3-4da2-be31-c1634bbe0eb8
    resourceVersion: "1634108"
    uid: 97e6116b-54ef-4d69-bbb6-347427ca7673
  spec:
    containers:
    - command:
      - kube-controller-manager
      - --allocate-node-cidrs=true
      - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --bind-address=127.0.0.1
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --cluster-cidr=10.244.0.0/16
      - --cluster-name=kubernetes
      - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      - --controllers=-nodelifecycle,*,bootstrapsigner,tokencleaner
      - --kubeconfig=/etc/kubernetes/controller-manager.conf
      - --leader-elect=true
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --root-ca-file=/etc/kubernetes/pki/ca.crt
      - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/12
      - --use-service-account-credentials=true
      image: registry.k8s.io/kube-controller-manager:v1.29.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-controller-manager
      resources:
        requests:
          cpu: 200m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /etc/pki
        name: etc-pki
        readOnly: true
      - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        name: flexvolume-dir
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /etc/kubernetes/controller-manager.conf
        name: kubeconfig
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-ddde9b
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /etc/pki
        type: DirectoryOrCreate
      name: etc-pki
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        type: DirectoryOrCreate
      name: flexvolume-dir
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /etc/kubernetes/controller-manager.conf
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:11:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:11:36Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5998bfc15fba77be363aa33e919d7c84a5bdaacf01802ff3b22f6e5cc56bc1b4
      image: registry.k8s.io/kube-controller-manager:v1.29.2
      imageID: registry.k8s.io/kube-controller-manager@sha256:4ac9c5b9e65bf9e42e0e9bd40c49d52915b872bf27736606007514bcef53cd93
      lastState:
        terminated:
          containerID: containerd://4582d4531ccfd722b46e7ac23fcd412fa972cc8ed59f9bd217ca0cfd2b23dfde
          exitCode: 1
          finishedAt: "2024-03-20T23:11:20Z"
          reason: Error
          startedAt: "2024-03-20T21:12:46Z"
      name: kube-controller-manager
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2024-03-20T23:11:21Z"
    hostIP: 192.168.1.106
    hostIPs:
    - ip: 192.168.1.106
    phase: Running
    podIP: 192.168.1.106
    podIPs:
    - ip: 192.168.1.106
    qosClass: Burstable
    startTime: "2024-03-19T15:52:20Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T15:04:40Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 65bbdcdfff
      k8s-app: kube-proxy
      pod-template-generation: "3"
    name: kube-proxy-ccpz5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: f5fe3eb1-5d29-4938-ba1f-0d953b5fe04e
    resourceVersion: "1397864"
    uid: d71f08b7-fc57-4cb7-8baf-014abf415cb6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-a6773f
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.29.2
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j8jn2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-a6773f
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-j8jn2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:10Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T15:04:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T15:04:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://269f7ef56a1ff66f037fba8043eb303b111c5b9453b41dcc463b8ad1105a8a53
      image: registry.k8s.io/kube-proxy:v1.29.2
      imageID: registry.k8s.io/kube-proxy@sha256:4a993783f8b8d6ec00281dd0bc334712fd7007316709f086a4a48bf250d24d08
      lastState:
        terminated:
          containerID: containerd://bc71a57447433ed987282d8e346d2ffb73f3453441bef825cab97b22fa2dcfef
          exitCode: 255
          finishedAt: "2023-09-19T17:00:22Z"
          reason: Unknown
          startedAt: "2024-03-17T15:04:57Z"
      name: kube-proxy
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:55:13Z"
    hostIP: 192.168.1.102
    hostIPs:
    - ip: 192.168.1.102
    phase: Running
    podIP: 192.168.1.102
    podIPs:
    - ip: 192.168.1.102
    qosClass: BestEffort
    startTime: "2024-03-17T15:04:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T12:36:51Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 65bbdcdfff
      k8s-app: kube-proxy
      pod-template-generation: "3"
    name: kube-proxy-hrzl9
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: f5fe3eb1-5d29-4938-ba1f-0d953b5fe04e
    resourceVersion: "1397249"
    uid: 149463a3-6eab-48ec-860a-b3716876cb9e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-ddde9b
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.29.2
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-62ss4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-ddde9b
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-62ss4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T12:36:51Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T12:36:51Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ee63d029b8a2724d9a06031c1b0d6922d3ea90c4e1375d4d19d05d21ebcb84ff
      image: registry.k8s.io/kube-proxy:v1.29.2
      imageID: registry.k8s.io/kube-proxy@sha256:4a993783f8b8d6ec00281dd0bc334712fd7007316709f086a4a48bf250d24d08
      lastState:
        terminated:
          containerID: containerd://6fc317fa65cc1cb44414a4bad21bf91fd9aebc03f62c5d3e6251a72f90646e65
          exitCode: 255
          finishedAt: "2024-03-19T15:52:06Z"
          reason: Unknown
          startedAt: "2024-03-17T14:46:27Z"
      name: kube-proxy
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:52:24Z"
    hostIP: 192.168.1.106
    hostIPs:
    - ip: 192.168.1.106
    phase: Running
    podIP: 192.168.1.106
    podIPs:
    - ip: 192.168.1.106
    qosClass: BestEffort
    startTime: "2024-03-17T12:36:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T13:14:43Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 65bbdcdfff
      k8s-app: kube-proxy
      pod-template-generation: "3"
    name: kube-proxy-pstxc
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: f5fe3eb1-5d29-4938-ba1f-0d953b5fe04e
    resourceVersion: "1653055"
    uid: 9ad0a2d0-f4e2-4493-847c-e2f90e657e3e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-75e4c5
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.29.2
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6hlsh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-75e4c5
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-6hlsh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T13:14:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T13:14:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2d242b3bda5213d3a5bc1a5d815c464288c7e42c86a76c28de2b22aef95a374c
      image: registry.k8s.io/kube-proxy:v1.29.2
      imageID: registry.k8s.io/kube-proxy@sha256:4a993783f8b8d6ec00281dd0bc334712fd7007316709f086a4a48bf250d24d08
      lastState:
        terminated:
          containerID: containerd://61d390876df68d1cc5f76cf7ac5fcba6b0f9b27da3f49fc9963a06ced8926e20
          exitCode: 255
          finishedAt: "2023-09-19T17:00:20Z"
          reason: Unknown
          startedAt: "2024-03-17T15:00:55Z"
      name: kube-proxy
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:55:04Z"
    hostIP: 192.168.1.100
    hostIPs:
    - ip: 192.168.1.100
    phase: Running
    podIP: 192.168.1.100
    podIPs:
    - ip: 192.168.1.100
    qosClass: BestEffort
    startTime: "2024-03-17T13:14:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T14:56:55Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 65bbdcdfff
      k8s-app: kube-proxy
      pod-template-generation: "3"
    name: kube-proxy-w2rzh
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: f5fe3eb1-5d29-4938-ba1f-0d953b5fe04e
    resourceVersion: "1397903"
    uid: c15b266e-47ed-40b1-933f-06142f8255a9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-76d48b
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.29.2
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8bmtn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-76d48b
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-8bmtn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T14:56:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T14:56:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4cb21262ab830d62aefdf15806082c2ab73ac6b0ad00f6473bb055bfe7150292
      image: registry.k8s.io/kube-proxy:v1.29.2
      imageID: registry.k8s.io/kube-proxy@sha256:4a993783f8b8d6ec00281dd0bc334712fd7007316709f086a4a48bf250d24d08
      lastState:
        terminated:
          containerID: containerd://e6b53081e3996b3a73c792e1955cc1d4141c237f8c5a18fc711ba2133a49d15b
          exitCode: 255
          finishedAt: "2023-09-19T17:00:23Z"
          reason: Unknown
          startedAt: "2024-03-17T14:57:00Z"
      name: kube-proxy
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:55:21Z"
    hostIP: 192.168.1.101
    hostIPs:
    - ip: 192.168.1.101
    phase: Running
    podIP: 192.168.1.101
    podIPs:
    - ip: 192.168.1.101
    qosClass: BestEffort
    startTime: "2024-03-17T14:56:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: bf0357f363d1a632a1cd9e6124fbcdeb
      kubernetes.io/config.mirror: bf0357f363d1a632a1cd9e6124fbcdeb
      kubernetes.io/config.seen: "2024-03-09T22:50:35.738175112+02:00"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-03-09T20:51:09Z"
    labels:
      component: kube-scheduler
      tier: control-plane
    name: kube-scheduler-ubuntu-host-ddde9b
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: ubuntu-host-ddde9b
      uid: ab930fb7-baf3-4da2-be31-c1634bbe0eb8
    resourceVersion: "1634107"
    uid: 4b320fa0-71dc-43ad-a010-9365b9a180b2
  spec:
    containers:
    - command:
      - kube-scheduler
      - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      - --bind-address=127.0.0.1
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      - --leader-elect=true
      image: registry.k8s.io/kube-scheduler:v1.29.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-scheduler
      resources:
        requests:
          cpu: 100m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/scheduler.conf
        name: kubeconfig
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-ddde9b
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/scheduler.conf
        type: FileOrCreate
      name: kubeconfig
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:11:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:11:36Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://adc8eb558e40d53aa8b47a8fd0ff687b99d3030fee2405570e3cad1c23e481e0
      image: registry.k8s.io/kube-scheduler:v1.29.2
      imageID: registry.k8s.io/kube-scheduler@sha256:108e51c8bcd2dcbd56462ef0d08a915bb19d956ad8bce167b6a2834ca92fe08f
      lastState:
        terminated:
          containerID: containerd://ad22cabca963d0688c1bc11b80bd4339d1c2ce1e227cc1984b4321f0e1bbdb08
          exitCode: 1
          finishedAt: "2024-03-20T23:11:25Z"
          reason: Error
          startedAt: "2024-03-20T21:12:51Z"
      name: kube-scheduler
      ready: true
      restartCount: 72
      started: true
      state:
        running:
          startedAt: "2024-03-20T23:11:26Z"
    hostIP: 192.168.1.106
    hostIPs:
    - ip: 192.168.1.106
    phase: Running
    podIP: 192.168.1.106
    podIPs:
    - ip: 192.168.1.106
    qosClass: Burstable
    startTime: "2024-03-19T15:52:20Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T15:07:31Z"
    generateName: raven-agent-ds-
    labels:
      app: raven-agent
      controller-revision-hash: 57878c8bc9
      pod-template-generation: "1"
    name: raven-agent-ds-58bqt
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: raven-agent-ds
      uid: 71c37f03-4746-4481-b487-148f7fcccb15
    resourceVersion: "1653057"
    uid: 2d60967b-ba36-417d-a33f-4943c30df944
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-75e4c5
    containers:
    - args:
      - --v=2
      - --vpn-driver=libreswan
      - --forward-node-ip=false
      - --nat-traversal=false
      - --metric-bind-addr=:10265
      - --health-probe-addr=:10275
      - --vpn-bind-port=:4500
      - --keep-alive-interval=15
      - --keep-alive-timeout=30
      - --proxy-metric-bind-addr=:10266
      - --proxy-internal-secure-addr=:10263
      - --proxy-internal-insecure-addr=:10264
      - --proxy-external-addr=:10262
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NODE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: VPN_CONNECTION_PSK
        valueFrom:
          secretKeyRef:
            key: vpn-connection-psk
            name: raven-agent-secret
      image: openyurt/raven-agent:0.4.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /healthz
          port: 10275
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 15
      name: raven-agent
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /readyz
          port: 10275
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 15
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/raven
        name: raven-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rl7lm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-75e4c5
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: raven-agent-account
    serviceAccountName: raven-agent-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/raven
        type: DirectoryOrCreate
      name: raven-dir
    - name: kube-api-access-rl7lm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T15:07:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T15:07:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1cd57140c41cd649efd15d35fc27033cb839f2fde3cdb686293ea3320d3ba9b9
      image: docker.io/openyurt/raven-agent:0.4.1
      imageID: docker.io/openyurt/raven-agent@sha256:7fa8b14457897a7c418268e04e1b374688cd3b42466098d70971c648dc085d2a
      lastState:
        terminated:
          containerID: containerd://8cafb59c59443d14448bfd0bea3126f4beb8fe1cf6c136aa6b112483ddf0b032
          exitCode: 255
          finishedAt: "2023-09-19T17:00:20Z"
          reason: Unknown
          startedAt: "2024-03-17T15:07:32Z"
      name: raven-agent
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:55:05Z"
    hostIP: 192.168.1.100
    hostIPs:
    - ip: 192.168.1.100
    phase: Running
    podIP: 192.168.1.100
    podIPs:
    - ip: 192.168.1.100
    qosClass: BestEffort
    startTime: "2024-03-17T15:07:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T10:50:55Z"
    generateName: raven-agent-ds-
    labels:
      app: raven-agent
      controller-revision-hash: 57878c8bc9
      pod-template-generation: "1"
    name: raven-agent-ds-799f7
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: raven-agent-ds
      uid: 71c37f03-4746-4481-b487-148f7fcccb15
    resourceVersion: "1397363"
    uid: 685dee1b-01fc-442f-bc4e-c679db30a559
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-ddde9b
    containers:
    - args:
      - --v=2
      - --vpn-driver=libreswan
      - --forward-node-ip=false
      - --nat-traversal=false
      - --metric-bind-addr=:10265
      - --health-probe-addr=:10275
      - --vpn-bind-port=:4500
      - --keep-alive-interval=15
      - --keep-alive-timeout=30
      - --proxy-metric-bind-addr=:10266
      - --proxy-internal-secure-addr=:10263
      - --proxy-internal-insecure-addr=:10264
      - --proxy-external-addr=:10262
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NODE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: VPN_CONNECTION_PSK
        valueFrom:
          secretKeyRef:
            key: vpn-connection-psk
            name: raven-agent-secret
      image: openyurt/raven-agent:0.4.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /healthz
          port: 10275
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 15
      name: raven-agent
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /readyz
          port: 10275
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 15
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/raven
        name: raven-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hwjcd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-ddde9b
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: raven-agent-account
    serviceAccountName: raven-agent-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/raven
        type: DirectoryOrCreate
      name: raven-dir
    - name: kube-api-access-hwjcd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T10:50:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T10:50:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e2010312002299528432131c904ac17ac87b22299dbee0bd6097268576d3526e
      image: docker.io/openyurt/raven-agent:0.4.1
      imageID: docker.io/openyurt/raven-agent@sha256:7fa8b14457897a7c418268e04e1b374688cd3b42466098d70971c648dc085d2a
      lastState:
        terminated:
          containerID: containerd://0b25fa375f6be23780e74879828d19f7eb843dab02dd2d3e092de28466b2cd59
          exitCode: 255
          finishedAt: "2024-03-19T15:52:06Z"
          reason: Unknown
          startedAt: "2024-03-17T14:46:27Z"
      name: raven-agent
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:52:24Z"
    hostIP: 192.168.1.106
    hostIPs:
    - ip: 192.168.1.106
    phase: Running
    podIP: 192.168.1.106
    podIPs:
    - ip: 192.168.1.106
    qosClass: BestEffort
    startTime: "2024-03-17T10:50:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T15:04:40Z"
    generateName: raven-agent-ds-
    labels:
      app: raven-agent
      controller-revision-hash: 57878c8bc9
      pod-template-generation: "1"
    name: raven-agent-ds-dkvzr
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: raven-agent-ds
      uid: 71c37f03-4746-4481-b487-148f7fcccb15
    resourceVersion: "1397963"
    uid: e2087bd0-00c3-4de4-8d9b-540451f331e9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-a6773f
    containers:
    - args:
      - --v=2
      - --vpn-driver=libreswan
      - --forward-node-ip=false
      - --nat-traversal=false
      - --metric-bind-addr=:10265
      - --health-probe-addr=:10275
      - --vpn-bind-port=:4500
      - --keep-alive-interval=15
      - --keep-alive-timeout=30
      - --proxy-metric-bind-addr=:10266
      - --proxy-internal-secure-addr=:10263
      - --proxy-internal-insecure-addr=:10264
      - --proxy-external-addr=:10262
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NODE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: VPN_CONNECTION_PSK
        valueFrom:
          secretKeyRef:
            key: vpn-connection-psk
            name: raven-agent-secret
      image: openyurt/raven-agent:0.4.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /healthz
          port: 10275
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 15
      name: raven-agent
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /readyz
          port: 10275
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 15
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/raven
        name: raven-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b5ztd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-a6773f
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: raven-agent-account
    serviceAccountName: raven-agent-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/raven
        type: DirectoryOrCreate
      name: raven-dir
    - name: kube-api-access-b5ztd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T15:04:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T15:04:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c4ad5de428446c113847d76e2b0d399f1761dfc3bdc511cd7d79c1bef1ec628d
      image: docker.io/openyurt/raven-agent:0.4.1
      imageID: docker.io/openyurt/raven-agent@sha256:7fa8b14457897a7c418268e04e1b374688cd3b42466098d70971c648dc085d2a
      lastState:
        terminated:
          containerID: containerd://0831f01a79ba1247566f17f79627068722f23933a428f3a30679bcaff7769a7f
          exitCode: 255
          finishedAt: "2023-09-19T17:00:22Z"
          reason: Unknown
          startedAt: "2024-03-17T15:05:00Z"
      name: raven-agent
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:55:14Z"
    hostIP: 192.168.1.102
    hostIPs:
    - ip: 192.168.1.102
    phase: Running
    podIP: 192.168.1.102
    podIPs:
    - ip: 192.168.1.102
    qosClass: BestEffort
    startTime: "2024-03-17T15:04:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T14:56:50Z"
    generateName: raven-agent-ds-
    labels:
      app: raven-agent
      controller-revision-hash: 57878c8bc9
      pod-template-generation: "1"
    name: raven-agent-ds-vsjzs
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: raven-agent-ds
      uid: 71c37f03-4746-4481-b487-148f7fcccb15
    resourceVersion: "1398022"
    uid: 091c0abe-65b4-4f9d-aea6-65cbdf9a5e92
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ubuntu-host-76d48b
    containers:
    - args:
      - --v=2
      - --vpn-driver=libreswan
      - --forward-node-ip=false
      - --nat-traversal=false
      - --metric-bind-addr=:10265
      - --health-probe-addr=:10275
      - --vpn-bind-port=:4500
      - --keep-alive-interval=15
      - --keep-alive-timeout=30
      - --proxy-metric-bind-addr=:10266
      - --proxy-internal-secure-addr=:10263
      - --proxy-internal-insecure-addr=:10264
      - --proxy-external-addr=:10262
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NODE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: VPN_CONNECTION_PSK
        valueFrom:
          secretKeyRef:
            key: vpn-connection-psk
            name: raven-agent-secret
      image: openyurt/raven-agent:0.4.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /healthz
          port: 10275
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 15
      name: raven-agent
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /readyz
          port: 10275
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 15
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/raven
        name: raven-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c8gl4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-76d48b
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: raven-agent-account
    serviceAccountName: raven-agent-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/raven
        type: DirectoryOrCreate
      name: raven-dir
    - name: kube-api-access-c8gl4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:55:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T14:56:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:56:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:56:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T14:56:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://200c591a84de05be9b66b159dfaedd787b3fbf370d4c7577ac075592fe1b8f9b
      image: docker.io/openyurt/raven-agent:0.4.1
      imageID: docker.io/openyurt/raven-agent@sha256:7fa8b14457897a7c418268e04e1b374688cd3b42466098d70971c648dc085d2a
      lastState:
        terminated:
          containerID: containerd://6d7ec3675312c95f4a0c99504de08cf51de681635ed02871c80315999616d413
          exitCode: 255
          finishedAt: "2023-09-19T17:00:23Z"
          reason: Unknown
          startedAt: "2024-03-17T14:56:55Z"
      name: raven-agent
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:55:23Z"
    hostIP: 192.168.1.101
    hostIPs:
    - ip: 192.168.1.101
    phase: Running
    podIP: 192.168.1.101
    podIPs:
    - ip: 192.168.1.101
    qosClass: BestEffort
    startTime: "2024-03-17T14:56:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 2dc8e43f0386c779e6252a9cb5ab8227
      kubernetes.io/config.mirror: 2dc8e43f0386c779e6252a9cb5ab8227
      kubernetes.io/config.seen: "2024-03-17T15:14:16.120685065+02:00"
      kubernetes.io/config.source: file
      openyurt.io/static-pod-hash: b8577c96f
    creationTimestamp: "2024-03-17T13:15:43Z"
    labels:
      k8s-app: yurt-hub
    name: yurt-hub-ubuntu-host-75e4c5
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: ubuntu-host-75e4c5
      uid: 6926799a-d222-473f-9d6c-3678042d0c7e
    resourceVersion: "1653053"
    uid: 497727d7-1434-4c5c-b50d-b432ed707932
  spec:
    containers:
    - command:
      - yurthub
      - --v=2
      - --bind-address=127.0.0.1
      - --server-addr=https://192.168.1.106:6443
      - --node-name=$(NODE_NAME)
      - --bootstrap-file=/var/lib/yurthub/bootstrap-hub.conf
      - --working-mode=edge
      - --namespace=kube-system
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: openyurt/yurthub:v1.4.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          path: /v1/healthz
          port: 10267
          scheme: HTTP
        initialDelaySeconds: 300
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: yurt-hub
      resources:
        limits:
          memory: 300Mi
        requests:
          cpu: 150m
          memory: 150Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/yurthub
        name: hub-dir
      - mountPath: /etc/kubernetes
        name: kubernetes
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-75e4c5
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/yurthub
        type: DirectoryOrCreate
      name: hub-dir
    - hostPath:
        path: /etc/kubernetes
        type: Directory
      name: kubernetes
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:39Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a0b4e13ca595f547b7078954d0d26c36055c0ec7bd8a70a9a714ee31f407a2b9
      image: docker.io/openyurt/yurthub:v1.4.0
      imageID: docker.io/openyurt/yurthub@sha256:40bd605e41afd4e0dd38b2f47c7fac0ff9ce5bf90dd621bb52571482ae560b0e
      lastState:
        terminated:
          containerID: containerd://7a683863e0d083d3d80a04d0d138fb2f7d648a5dbcfb4b9ad66d9128a798f1a2
          exitCode: 255
          finishedAt: "2023-09-19T17:00:20Z"
          reason: Unknown
          startedAt: "2024-03-17T15:06:01Z"
      name: yurt-hub
      ready: true
      restartCount: 8
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:54:38Z"
    hostIP: 192.168.1.100
    hostIPs:
    - ip: 192.168.1.100
    phase: Running
    podIP: 192.168.1.100
    podIPs:
    - ip: 192.168.1.100
    qosClass: Burstable
    startTime: "2024-03-19T15:54:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: c0d76364c0978b7d1a1230540c8ba60f
      kubernetes.io/config.mirror: c0d76364c0978b7d1a1230540c8ba60f
      kubernetes.io/config.seen: "2024-03-17T15:56:17.500604396+02:00"
      kubernetes.io/config.source: file
      openyurt.io/static-pod-hash: b8577c96f
    creationTimestamp: "2024-03-17T13:57:34Z"
    labels:
      k8s-app: yurt-hub
    name: yurt-hub-ubuntu-host-76d48b
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: ubuntu-host-76d48b
      uid: 3cfd0bae-e74c-4c50-bd47-cf8bb6b56de1
    resourceVersion: "1397773"
    uid: 9b837e93-7fbc-4931-b765-76b3d5f60ed4
  spec:
    containers:
    - command:
      - yurthub
      - --v=2
      - --bind-address=127.0.0.1
      - --server-addr=https://192.168.1.106:6443
      - --node-name=$(NODE_NAME)
      - --bootstrap-file=/var/lib/yurthub/bootstrap-hub.conf
      - --working-mode=edge
      - --namespace=kube-system
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: openyurt/yurthub:v1.4.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          path: /v1/healthz
          port: 10267
          scheme: HTTP
        initialDelaySeconds: 300
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: yurt-hub
      resources:
        limits:
          memory: 300Mi
        requests:
          cpu: 150m
          memory: 150Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/yurthub
        name: hub-dir
      - mountPath: /etc/kubernetes
        name: kubernetes
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-76d48b
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/yurthub
        type: DirectoryOrCreate
      name: hub-dir
    - hostPath:
        path: /etc/kubernetes
        type: Directory
      name: kubernetes
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:41Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:41Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:41Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://fde20f305d4ebb5796080cca009eebc734e577b74f2e2c95808bf8ab7baaf78f
      image: docker.io/openyurt/yurthub:v1.4.0
      imageID: docker.io/openyurt/yurthub@sha256:40bd605e41afd4e0dd38b2f47c7fac0ff9ce5bf90dd621bb52571482ae560b0e
      lastState:
        terminated:
          containerID: containerd://ff8c7f4a5b25b81a1259ba96c4fd5471befbeb8a726e1888c5bb286d82f8fdb8
          exitCode: 255
          finishedAt: "2023-09-19T17:00:23Z"
          reason: Unknown
          startedAt: "2024-03-17T14:56:20Z"
      name: yurt-hub
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:54:40Z"
    hostIP: 192.168.1.101
    hostIPs:
    - ip: 192.168.1.101
    phase: Running
    podIP: 192.168.1.101
    podIPs:
    - ip: 192.168.1.101
    qosClass: Burstable
    startTime: "2024-03-19T15:54:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: c20a11b058e3521fa6885df6d7bf254a
      kubernetes.io/config.mirror: c20a11b058e3521fa6885df6d7bf254a
      kubernetes.io/config.seen: "2024-03-17T15:59:49.656521074+02:00"
      kubernetes.io/config.source: file
      openyurt.io/static-pod-hash: b8577c96f
    creationTimestamp: "2024-03-17T14:01:27Z"
    labels:
      k8s-app: yurt-hub
    name: yurt-hub-ubuntu-host-a6773f
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: ubuntu-host-a6773f
      uid: e6f51315-c5e6-40bb-ad1f-56eb93c55600
    resourceVersion: "1397807"
    uid: 4f7f8b46-aa49-4828-9e86-8dbf970e8831
  spec:
    containers:
    - command:
      - yurthub
      - --v=2
      - --bind-address=127.0.0.1
      - --server-addr=https://192.168.1.106:6443
      - --node-name=$(NODE_NAME)
      - --bootstrap-file=/var/lib/yurthub/bootstrap-hub.conf
      - --working-mode=edge
      - --namespace=kube-system
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: openyurt/yurthub:v1.4.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          path: /v1/healthz
          port: 10267
          scheme: HTTP
        initialDelaySeconds: 300
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: yurt-hub
      resources:
        limits:
          memory: 300Mi
        requests:
          cpu: 150m
          memory: 150Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/yurthub
        name: hub-dir
      - mountPath: /etc/kubernetes
        name: kubernetes
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ubuntu-host-a6773f
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/yurthub
        type: DirectoryOrCreate
      name: hub-dir
    - hostPath:
        path: /etc/kubernetes
        type: Directory
      name: kubernetes
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:40Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:54:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://973c9333dc284f77f16bcd4d50c71b692227df783caaf9c22ea2dbb51a0cb934
      image: docker.io/openyurt/yurthub:v1.4.0
      imageID: docker.io/openyurt/yurthub@sha256:40bd605e41afd4e0dd38b2f47c7fac0ff9ce5bf90dd621bb52571482ae560b0e
      lastState:
        terminated:
          containerID: containerd://6517fff1353199afc36bac16b990a6d7f1f7f101870fec4138a9441dab4e5e9a
          exitCode: 255
          finishedAt: "2023-09-19T17:00:22Z"
          reason: Unknown
          startedAt: "2024-03-17T15:04:10Z"
      name: yurt-hub
      ready: true
      restartCount: 9
      started: true
      state:
        running:
          startedAt: "2024-03-19T15:54:39Z"
    hostIP: 192.168.1.102
    hostIPs:
    - ip: 192.168.1.102
    phase: Running
    podIP: 192.168.1.102
    podIPs:
    - ip: 192.168.1.102
    qosClass: Burstable
    startTime: "2024-03-19T15:54:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T10:46:20Z"
    generateName: yurt-manager-6bffd94c4b-
    labels:
      app.kubernetes.io/instance: yurt-manager
      app.kubernetes.io/name: yurt-manager
      pod-template-hash: 6bffd94c4b
    name: yurt-manager-6bffd94c4b-2bg95
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: yurt-manager-6bffd94c4b
      uid: 20f14c83-f093-4657-b8ab-dd1f63e17421
    resourceVersion: "1634397"
    uid: 859d6b9c-e0b7-41fe-9d94-78e5c22b6db3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: Exists
    containers:
    - args:
      - --metrics-addr=:10271
      - --health-probe-addr=:10272
      - --webhook-port=10273
      - --logtostderr=true
      - --v=4
      - --working-namespace=kube-system
      - --leader-elect-resource-name=cloud-yurt-manager
      - --controllers=*
      command:
      - /usr/local/bin/yurt-manager
      image: openyurt/yurt-manager:v1.4.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10272
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      name: yurt-manager
      ports:
      - containerPort: 10273
        name: webhook-server
        protocol: TCP
      - containerPort: 10271
        name: metrics
        protocol: TCP
      - containerPort: 10272
        name: health
        protocol: TCP
      readinessProbe:
        failureThreshold: 2
        httpGet:
          path: /readyz
          port: 10272
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        limits:
          cpu: "2"
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nlfxg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ubuntu-host-ddde9b
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: yurt-manager
    serviceAccountName: yurt-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-nlfxg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T15:52:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T10:46:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:14:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-20T23:14:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T10:46:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8fd654ab5a8ed287e5c9af18b255417e0152066792f98d3aa80ac608d9e31580
      image: docker.io/openyurt/yurt-manager:v1.4.0
      imageID: docker.io/openyurt/yurt-manager@sha256:12d4d9eb734164c7a3ece4aee9c59dce47ca91d786fbe8d34463b6872c88d2a0
      lastState:
        terminated:
          containerID: containerd://51bef826f3d69a4de57725de68762b294cc300f0d81b2ff71989bb932280c6fc
          exitCode: 1
          finishedAt: "2024-03-20T23:12:47Z"
          reason: Error
          startedAt: "2024-03-20T23:12:44Z"
      name: yurt-manager
      ready: true
      restartCount: 22
      started: true
      state:
        running:
          startedAt: "2024-03-20T23:13:29Z"
    hostIP: 192.168.1.106
    hostIPs:
    - ip: 192.168.1.106
    phase: Running
    podIP: 10.244.0.26
    podIPs:
    - ip: 10.244.0.26
    qosClass: Burstable
    startTime: "2024-03-17T10:46:20Z"
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-03-09T20:50:34Z"
    labels:
      component: apiserver
      provider: kubernetes
      service.edgemesh.kubeedge.io/service-proxy-name: ""
    name: kubernetes
    namespace: default
    resourceVersion: "63006"
    uid: b7a7cd41-6554-4adc-ba73-fe227404e62c
  spec:
    clusterIP: 10.96.0.1
    clusterIPs:
    - 10.96.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-03-09T20:50:35Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "1429486"
    uid: d02dc1e6-5985-4710-a0e6-26fc03c76cbd
  spec:
    clusterIP: 10.96.0.10
    clusterIPs:
    - 10.96.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: yurt-manager
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2024-03-17T10:46:20Z"
    labels:
      app.kubernetes.io/instance: yurt-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: yurt-manager
      app.kubernetes.io/version: 1.4.0
      helm.sh/chart: yurt-manager-1.4.3
    name: yurt-manager-webhook-service
    namespace: kube-system
    resourceVersion: "1001012"
    uid: 1ff3018e-44d0-40ff-8eaa-14e3f52a9f87
  spec:
    clusterIP: 10.101.33.13
    clusterIPs:
    - 10.101.33.13
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 10273
    - name: metrics
      port: 10271
      protocol: TCP
      targetPort: 10271
    selector:
      app.kubernetes.io/instance: yurt-manager
      app.kubernetes.io/name: yurt-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"app":"flannel","k8s-app":"flannel","tier":"node"},"name":"kube-flannel-ds","namespace":"kube-flannel"},"spec":{"selector":{"matchLabels":{"app":"flannel","k8s-app":"flannel"}},"template":{"metadata":{"labels":{"app":"flannel","k8s-app":"flannel","tier":"node"}},"spec":{"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"kubernetes.io/os","operator":"In","values":["linux"]}]}]}}},"containers":[{"args":["--ip-masq","--kube-subnet-mgr"],"command":["/opt/bin/flanneld"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"EVENT_QUEUE_DEPTH","value":"5000"}],"image":"docker.io/flannel/flannel:v0.24.3","name":"kube-flannel","resources":{"requests":{"cpu":"100m","memory":"50Mi"}},"securityContext":{"capabilities":{"add":["NET_ADMIN","NET_RAW"]},"privileged":false},"volumeMounts":[{"mountPath":"/run/flannel","name":"run"},{"mountPath":"/etc/kube-flannel/","name":"flannel-cfg"},{"mountPath":"/run/xtables.lock","name":"xtables-lock"}]}],"hostNetwork":true,"initContainers":[{"args":["-f","/flannel","/opt/cni/bin/flannel"],"command":["cp"],"image":"docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1","name":"install-cni-plugin","volumeMounts":[{"mountPath":"/opt/cni/bin","name":"cni-plugin"}]},{"args":["-f","/etc/kube-flannel/cni-conf.json","/etc/cni/net.d/10-flannel.conflist"],"command":["cp"],"image":"docker.io/flannel/flannel:v0.24.3","name":"install-cni","volumeMounts":[{"mountPath":"/etc/cni/net.d","name":"cni"},{"mountPath":"/etc/kube-flannel/","name":"flannel-cfg"}]}],"priorityClassName":"system-node-critical","serviceAccountName":"flannel","tolerations":[{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/run/flannel"},"name":"run"},{"hostPath":{"path":"/opt/cni/bin"},"name":"cni-plugin"},{"hostPath":{"path":"/etc/cni/net.d"},"name":"cni"},{"configMap":{"name":"kube-flannel-cfg"},"name":"flannel-cfg"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"xtables-lock"}]}}}}
    creationTimestamp: "2024-03-09T20:52:24Z"
    generation: 3
    labels:
      app: flannel
      k8s-app: flannel
      tier: node
    name: kube-flannel-ds
    namespace: kube-flannel
    resourceVersion: "1653060"
    uid: e54a963e-05f8-41d2-83b2-f61755a3cbd7
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: flannel
        k8s-app: flannel
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: flannel
          k8s-app: flannel
          tier: node
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: In
                  values:
                  - linux
        containers:
        - args:
          - --ip-masq
          - --kube-subnet-mgr
          command:
          - /opt/bin/flanneld
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: EVENT_QUEUE_DEPTH
            value: "5000"
          image: docker.io/flannel/flannel:v0.24.3
          imagePullPolicy: IfNotPresent
          name: kube-flannel
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
              - NET_RAW
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /run/flannel
            name: run
          - mountPath: /etc/kube-flannel/
            name: flannel-cfg
          - mountPath: /run/xtables.lock
            name: xtables-lock
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - args:
          - -f
          - /flannel
          - /opt/cni/bin/flannel
          command:
          - cp
          image: docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
          imagePullPolicy: IfNotPresent
          name: install-cni-plugin
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/cni/bin
            name: cni-plugin
        - args:
          - -f
          - /etc/kube-flannel/cni-conf.json
          - /etc/cni/net.d/10-flannel.conflist
          command:
          - cp
          image: docker.io/flannel/flannel:v0.24.3
          imagePullPolicy: IfNotPresent
          name: install-cni
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cni/net.d
            name: cni
          - mountPath: /etc/kube-flannel/
            name: flannel-cfg
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: flannel
        serviceAccountName: flannel
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /run/flannel
            type: ""
          name: run
        - hostPath:
            path: /opt/cni/bin
            type: ""
          name: cni-plugin
        - hostPath:
            path: /etc/cni/net.d
            type: ""
          name: cni
        - configMap:
            defaultMode: 420
            name: kube-flannel-cfg
          name: flannel-cfg
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 3
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "3"
    creationTimestamp: "2024-03-09T20:50:35Z"
    generation: 3
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "1653056"
    uid: f5fe3eb1-5d29-4938-ba1f-0d953b5fe04e
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --config=/var/lib/kube-proxy/config.conf
          - --hostname-override=$(NODE_NAME)
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.k8s.io/kube-proxy:v1.29.2
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 3
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: raven-agent
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2024-03-17T10:50:55Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: raven-agent-ds
    namespace: kube-system
    resourceVersion: "1653058"
    uid: 71c37f03-4746-4481-b487-148f7fcccb15
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: raven-agent
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: raven-agent
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: type
                  operator: NotIn
                  values:
                  - virtual-kubelet
        containers:
        - args:
          - --v=2
          - --vpn-driver=libreswan
          - --forward-node-ip=false
          - --nat-traversal=false
          - --metric-bind-addr=:10265
          - --health-probe-addr=:10275
          - --vpn-bind-port=:4500
          - --keep-alive-interval=15
          - --keep-alive-timeout=30
          - --proxy-metric-bind-addr=:10266
          - --proxy-internal-secure-addr=:10263
          - --proxy-internal-insecure-addr=:10264
          - --proxy-external-addr=:10262
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: NODE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIP
          - name: VPN_CONNECTION_PSK
            valueFrom:
              secretKeyRef:
                key: vpn-connection-psk
                name: raven-agent-secret
          image: openyurt/raven-agent:0.4.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /healthz
              port: 10275
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 15
          name: raven-agent
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /readyz
              port: 10275
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 15
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/raven
            name: raven-dir
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: raven-agent-account
        serviceAccountName: raven-agent-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /var/lib/raven
            type: DirectoryOrCreate
          name: raven-dir
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 5%
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-09T20:50:35Z"
    generation: 1
    labels:
      k8s-app: kube-dns
    name: coredns
    namespace: kube-system
    resourceVersion: "1634310"
    uid: 953670b6-c167-4a3c-b774-f47358eef0c3
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: registry.k8s.io/coredns/coredns:v1.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2024-03-09T20:51:27Z"
      lastUpdateTime: "2024-03-09T20:52:26Z"
      message: ReplicaSet "coredns-76f75df574" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-20T23:13:37Z"
      lastUpdateTime: "2024-03-20T23:13:37Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: yurt-manager
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2024-03-17T10:46:20Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: yurt-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: yurt-manager
      app.kubernetes.io/version: 1.4.0
      helm.sh/chart: yurt-manager-1.4.3
    name: yurt-manager
    namespace: kube-system
    resourceVersion: "1634401"
    uid: 9c9ccf31-e827-4a09-842e-4a7abd5c67b8
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: yurt-manager
        app.kubernetes.io/name: yurt-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: yurt-manager
          app.kubernetes.io/name: yurt-manager
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
        containers:
        - args:
          - --metrics-addr=:10271
          - --health-probe-addr=:10272
          - --webhook-port=10273
          - --logtostderr=true
          - --v=4
          - --working-namespace=kube-system
          - --leader-elect-resource-name=cloud-yurt-manager
          - --controllers=*
          command:
          - /usr/local/bin/yurt-manager
          image: openyurt/yurt-manager:v1.4.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10272
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: yurt-manager
          ports:
          - containerPort: 10273
            name: webhook-server
            protocol: TCP
          - containerPort: 10271
            name: metrics
            protocol: TCP
          - containerPort: 10272
            name: health
            protocol: TCP
          readinessProbe:
            failureThreshold: 2
            httpGet:
              path: /readyz
              port: 10272
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: "2"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: yurt-manager
        serviceAccountName: yurt-manager
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-17T10:46:20Z"
      lastUpdateTime: "2024-03-17T10:47:30Z"
      message: ReplicaSet "yurt-manager-6bffd94c4b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-20T23:14:31Z"
      lastUpdateTime: "2024-03-20T23:14:31Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-09T20:51:27Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 76f75df574
    name: coredns-76f75df574
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: 953670b6-c167-4a3c-b774-f47358eef0c3
    resourceVersion: "1634309"
    uid: 6ca83217-cda2-4947-a447-8924ff57dff6
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 76f75df574
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 76f75df574
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: registry.k8s.io/coredns/coredns:v1.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: yurt-manager
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2024-03-17T10:46:20Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: yurt-manager
      app.kubernetes.io/name: yurt-manager
      pod-template-hash: 6bffd94c4b
    name: yurt-manager-6bffd94c4b
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: yurt-manager
      uid: 9c9ccf31-e827-4a09-842e-4a7abd5c67b8
    resourceVersion: "1634400"
    uid: 20f14c83-f093-4657-b8ab-dd1f63e17421
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: yurt-manager
        app.kubernetes.io/name: yurt-manager
        pod-template-hash: 6bffd94c4b
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: yurt-manager
          app.kubernetes.io/name: yurt-manager
          pod-template-hash: 6bffd94c4b
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
        containers:
        - args:
          - --metrics-addr=:10271
          - --health-probe-addr=:10272
          - --webhook-port=10273
          - --logtostderr=true
          - --v=4
          - --working-namespace=kube-system
          - --leader-elect-resource-name=cloud-yurt-manager
          - --controllers=*
          command:
          - /usr/local/bin/yurt-manager
          image: openyurt/yurt-manager:v1.4.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10272
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: yurt-manager
          ports:
          - containerPort: 10273
            name: webhook-server
            protocol: TCP
          - containerPort: 10271
            name: metrics
            protocol: TCP
          - containerPort: 10272
            name: health
            protocol: TCP
          readinessProbe:
            failureThreshold: 2
            httpGet:
              path: /readyz
              port: 10272
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: "2"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: yurt-manager
        serviceAccountName: yurt-manager
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
kind: List
metadata:
  resourceVersion: ""
